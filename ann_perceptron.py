# -*- coding: utf-8 -*-
"""ANN-Perceptron

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lua78DKXivG8TN0MhO4t9j4xUDpJUCy5

# Artificial Neural Network (ANN): Perceptron

**Aluno**: Maruan Biasi El Achkar               
**GitHub**: https://github.com/MachineNeyarning/ANN-Perceptron

------------

# 1. Problema das portas lógicas AND, OR
"""

import numpy as np

np.random.seed(42)

class Perceptron:
    def __init__(self, input_size, lr=0.01, epochs=1000):
        self.lr = lr # learning rate
        self.epochs = epochs # quantidade de "rodadas" de treino
        self.weights = np.random.randn(input_size) # peso inicial aleatorio
        self.bias = np.random.randn() # bias aleatorio

    def activation(self, x):
        return np.where(x >= 0, 1, 0) # se x for maior que 0, retorna 1. senao retorna 0

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias # weighted sum das entradas + bias
        return self.activation(linear_output)

    def fit(self, X, y):
        for _ in range(self.epochs): # rodadas
            for xi, target in zip(X, y):
                linear_output = np.dot(xi, self.weights) + self.bias
                y_pred = self.activation(linear_output)
                # ----
                update = self.lr * (target - y_pred) # erro vezes learning rate
                self.weights += update * xi # ajuste dos pesos
                self.bias += update # ajuste do bias



# calculadora de accuracy, compara a truth table com a predicao
def accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return (correct / total) * 100


# AND GATE
X_and = np.array([[0,0], [0,1], [1,0], [1,1]])
y_and = np.array([0, 0, 0, 1])

# OR GATE
X_or = np.array([[0,0], [0,1], [1,0], [1,1]])
y_or = np.array([0, 1, 1, 1])

# XOR GATE
X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])
y_xor = np.array([0, 1, 1, 0])

# treino
model = Perceptron(input_size=2, lr=0.1, epochs=10)

# fit AND
model.fit(X_and, y_and)

y_and_predict = model.predict(X_and)
print("-----------------------------------------")
print("- AND GATE")
print(f"- Acurracy: {accuracy(y_and, y_and_predict):.2f}%")
print("- Predictions:", y_and_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")

# fit OR
model.fit(X_or, y_or)

y_or_predict = model.predict(X_or)
print("-----------------------------------------")
print("- OR GATE")
print(f"- Acurracy: {accuracy(y_or, y_or_predict):.2f}%")
print("- Predictions:", y_or_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")

# fit XOR
model.fit(X_xor, y_xor)

y_xor_predict = model.predict(X_xor)
print("-----------------------------------------")
print("- XOR GATE")
print(f"- Acurracy: {accuracy(y_xor, y_xor_predict):.2f}%")
print("- Predictions:", y_xor_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")

"""# 2. Repita o exercício 1 sobre o problema das portas lógicas utilizando a Regra de Aprendizado Delta"""

import numpy as np

import numpy as np

np.random.seed(42)

class Perceptron:
    def __init__(self, input_size, lr=0.01, epochs=1000):
        self.lr = lr  # learning rate
        self.epochs = epochs  # épocas de treino
        self.weights = np.random.randn(input_size)  # pesos iniciais
        self.bias = np.random.randn()  # bias inicial

    # Função de ativação binária (para inferência)
    def activation(self, x):
        return np.where(x >= 0, 1, 0)

    # Predição binária (usa degrau)
    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return self.activation(linear_output)

    # Regra Perceptron
    def fit_perceptron(self, X, y):
        for _ in range(self.epochs):
            for xi, target in zip(X, y):
                linear_output = np.dot(xi, self.weights) + self.bias
                y_pred_bin = self.activation(linear_output)
                update = self.lr * (target - y_pred_bin)
                self.weights += update * xi
                self.bias += update

    # Regra Delta Widrow–Hoff
    # atualiza os pesos usando saida linear
    # w <- w + lr * (target - y_hat) * x
    # b <- b + lr * (target - y_hat)
    def fit_delta(self, X, y):
        for _ in range(self.epochs):
            for xi, target in zip(X, y):
                y_hat = np.dot(xi, self.weights) + self.bias  # saída linear
                error = target - y_hat # erro contínuo
                self.weights += self.lr * error * xi # atualização Delta
                self.bias += self.lr * error

# calculadora de accuracy, compara a truth table com a predição
def accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return (correct / total) * 100

# Tabelas verdade
X_and = np.array([[0,0], [0,1], [1,0], [1,1]])
y_and = np.array([0, 0, 0, 1])

X_or = np.array([[0,0], [0,1], [1,0], [1,1]])
y_or = np.array([0, 1, 1, 1])

X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])
y_xor = np.array([0, 1, 1, 0])

# Treino com Regra Delta
model = Perceptron(input_size=2, lr=0.1, epochs=50)

# AND com Delta
model.fit_delta(X_and, y_and)

y_and_predict = model.predict(X_and)
print("-----------------------------------------")
print("- AND GATE (Delta)")
print(f"- Accuracy: {accuracy(y_and, y_and_predict):.2f}%")
print("- Predictions:", y_and_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")

# OR com Delta
model.fit_delta(X_or, y_or)

y_or_predict = model.predict(X_or)
print("-----------------------------------------")
print("- OR GATE (Delta)")
print(f"- Accuracy: {accuracy(y_or, y_or_predict):.2f}%")
print("- Predictions:", y_or_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")

# XOR com Delta
model.fit_delta(X_xor, y_xor)

y_xor_predict = model.predict(X_xor)
print("-----------------------------------------")
print("- XOR GATE (Delta)")
print(f"- Accuracy: {accuracy(y_xor, y_xor_predict):.2f}%")
print("- Predictions:", y_xor_predict)
print("- Weights:", model.weights)
print("- Bias:", model.bias)
print("-----------------------------------------")